{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the dependancies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "import re\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import sys\n",
    "import pandas.core.algorithms as algos\n",
    "pd.pandas.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Vishal\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Dataset\n",
    "#df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
    "#test_data = pd.read_csv('E:\\Fake News Detection/test.csv')\n",
    "#submit = pd.read_csv('E:\\Fake News Detection/submit.csv')\n",
    "#df = pd.read_csv('train.csv')\n",
    "df = pd.read_csv('E:\\Fake News Detection/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the train and test data across rows\n",
    "#df = pd.concat([train_data, test_data], axis = 'rows')\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Darrell Lucus House Dem Aide: We Didn’t Even S...\n",
       "1    Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...\n",
       "2    Consortiumnews.com Why the Truth Might Get You...\n",
       "3    Jessica Purkiss 15 Civilians Killed In Single ...\n",
       "4    Howard Portnoy Iranian woman jailed for fictio...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merging the \"author\" and \"title\" columns to form \"Content\"\n",
    "df['content'] = df['author']+' '+df['title'] #+df['text']\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the data & label\n",
    "X = df.drop(columns='label', axis=1)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function \"clean\" to stem to the words in content\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer\n",
    "def clean(text):\n",
    "    text=\"\".join([re.sub('[^a-zA-Z]',' ',char) for char in text])\n",
    "    text=text.lower()\n",
    "    text=text.split()\n",
    "    text=[stemmer.stem(word) for word in text if word not in set(stopwords.words(\"english\"))]\n",
    "    text=\" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        darrel lucu hous dem aid even see comey letter...\n",
      "1        daniel j flynn flynn hillari clinton big woman...\n",
      "2                   consortiumnew com truth might get fire\n",
      "3        jessica purkiss civilian kill singl us airstri...\n",
      "4        howard portnoy iranian woman jail fiction unpu...\n",
      "                               ...                        \n",
      "20795    jerom hudson rapper trump poster child white s...\n",
      "20796    benjamin hoffman n f l playoff schedul matchup...\n",
      "20797    michael j de la merc rachel abram maci said re...\n",
      "20798    alex ansari nato russia hold parallel exercis ...\n",
      "20799                            david swanson keep f aliv\n",
      "Name: content, Length: 20800, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating lables and values\n",
    "X = df['content'].values\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['darrel lucu hous dem aid even see comey letter jason chaffetz tweet'\n",
      " 'daniel j flynn flynn hillari clinton big woman campu breitbart'\n",
      " 'consortiumnew com truth might get fire' ...\n",
      " 'michael j de la merc rachel abram maci said receiv takeov approach hudson bay new york time'\n",
      " 'alex ansari nato russia hold parallel exercis balkan'\n",
      " 'david swanson keep f aliv']\n",
      "[1 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the textual data into numerical data using vertorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X)\n",
    "\n",
    "X = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15686)\t0.28485063562728646\n",
      "  (0, 13473)\t0.2565896679337957\n",
      "  (0, 8909)\t0.3635963806326075\n",
      "  (0, 8630)\t0.29212514087043684\n",
      "  (0, 7692)\t0.24785219520671603\n",
      "  (0, 7005)\t0.21874169089359144\n",
      "  (0, 4973)\t0.233316966909351\n",
      "  (0, 3792)\t0.2705332480845492\n",
      "  (0, 3600)\t0.3598939188262559\n",
      "  (0, 2959)\t0.2468450128533713\n",
      "  (0, 2483)\t0.3676519686797209\n",
      "  (0, 267)\t0.27010124977708766\n",
      "  (1, 16799)\t0.30071745655510157\n",
      "  (1, 6816)\t0.1904660198296849\n",
      "  (1, 5503)\t0.7143299355715573\n",
      "  (1, 3568)\t0.26373768806048464\n",
      "  (1, 2813)\t0.19094574062359204\n",
      "  (1, 2223)\t0.3827320386859759\n",
      "  (1, 1894)\t0.15521974226349364\n",
      "  (1, 1497)\t0.2939891562094648\n",
      "  (2, 15611)\t0.41544962664721613\n",
      "  (2, 9620)\t0.49351492943649944\n",
      "  (2, 5968)\t0.3474613386728292\n",
      "  (2, 5389)\t0.3866530551182615\n",
      "  (2, 3103)\t0.46097489583229645\n",
      "  :\t:\n",
      "  (20797, 13122)\t0.2482526352197606\n",
      "  (20797, 12344)\t0.27263457663336677\n",
      "  (20797, 12138)\t0.24778257724396507\n",
      "  (20797, 10306)\t0.08038079000566466\n",
      "  (20797, 9588)\t0.174553480255222\n",
      "  (20797, 9518)\t0.2954204003420313\n",
      "  (20797, 8988)\t0.36160868928090795\n",
      "  (20797, 8364)\t0.22322585870464118\n",
      "  (20797, 7042)\t0.21799048897828688\n",
      "  (20797, 3643)\t0.21155500613623743\n",
      "  (20797, 1287)\t0.33538056804139865\n",
      "  (20797, 699)\t0.30685846079762347\n",
      "  (20797, 43)\t0.29710241860700626\n",
      "  (20798, 13046)\t0.22363267488270608\n",
      "  (20798, 11052)\t0.4460515589182236\n",
      "  (20798, 10177)\t0.3192496370187028\n",
      "  (20798, 6889)\t0.32496285694299426\n",
      "  (20798, 5032)\t0.4083701450239529\n",
      "  (20798, 1125)\t0.4460515589182236\n",
      "  (20798, 588)\t0.3112141524638974\n",
      "  (20798, 350)\t0.28446937819072576\n",
      "  (20799, 14852)\t0.5677577267055112\n",
      "  (20799, 8036)\t0.45983893273780013\n",
      "  (20799, 3623)\t0.37927626273066584\n",
      "  (20799, 377)\t0.5677577267055112\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting test and train data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y, random_state=42)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the training data :  0.9874399038461539\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score on the training data\n",
    "X_train_prediction = model.predict(X_train)\n",
    "training_data_accuracy = accuracy_score(X_train_prediction, y_train)\n",
    "print('Accuracy score of the training data : ', training_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the testing data : 0.9752403846153846\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score on test data\n",
    "X_test_pred = model.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(X_test_pred, y_test)\n",
    "print ('Accuracy score of the testing data :', test_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9711538461538461"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implementing Bagging ensemble on Logistic Regression Classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagging_clf = BaggingClassifier(base_estimator = LogisticRegression(), n_estimators = 100, max_features = 15000, random_state = 42)\n",
    "bagging_clf.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'use_label_encoder': False,\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'callbacks': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'early_stopping_rounds': None,\n",
       " 'enable_categorical': False,\n",
       " 'eval_metric': None,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'grow_policy': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_bin': None,\n",
       " 'max_cat_to_onehot': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'max_leaves': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'predictor': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'sampling_method': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric='auc', gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, objective='binary:logistic',\n",
       "              predictor='auto', random_state=0, reg_alpha=0, ...)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgboost = xgboost.XGBClassifier(n_estimators = 100,\n",
    "                                     eval_metric = 'auc')\n",
    "#eval_set =[(X_val, y_val)]\n",
    "model_xgboost.fit(X_train,\n",
    "                  y_train,\n",
    "                  #early_stopping_rounds=10,\n",
    "                  #eval_set=eval_set,\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Train: 0.9994\n",
      "AUC Test: 0.9950\n",
      "Accuracy Train: 0.9947\n",
      "Accuracy Test: 0.9875\n"
     ]
    }
   ],
   "source": [
    "y_train_pred=model_xgboost.predict_proba(X_train)[:,1]\n",
    "y_test_pred=model_xgboost.predict_proba(X_test)[:,1]\n",
    "print(\"AUC Train: {:.4f}\\nAUC Test: {:.4f}\".format(roc_auc_score(y_train, y_train_pred),\n",
    "                                                    roc_auc_score(y_test, y_test_pred)))\n",
    "\n",
    "y_train_pred=model_xgboost.predict(X_train)\n",
    "y_test_pred=model_xgboost.predict(X_test)\n",
    "print(\"Accuracy Train: {:.4f}\\nAccuracy Test: {:.4f}\".format(accuracy_score(y_train, y_train_pred),\n",
    "                                                    accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation result without hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "score = cross_val_score(model_xgboost, X, y, cv =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9891826923076923"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'use_label_encoder': False,\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'callbacks': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'early_stopping_rounds': None,\n",
       " 'enable_categorical': False,\n",
       " 'eval_metric': None,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'grow_policy': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_bin': None,\n",
       " 'max_cat_to_onehot': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'max_leaves': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'predictor': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'sampling_method': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the parameters for the defaul XGBoost model\n",
    "XGBClassifier().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning on XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting superset of parameters to train on\n",
    "params = {\n",
    "    \"learning_rate\" : [0.01, 0.10, 0.25, 0.5, 1],\n",
    "    \"max_depth\" : [3,5,10],\n",
    "    \"min_child_weight\" : [1,3,5,7, 10, 13,17, 20],\n",
    "    \"gamma\" : [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    \"colsample_bytree\" : [0.3, 0.4, 0.5, 0.7, 1],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning using Randomized Search CV\n",
    "random_search =  RandomizedSearchCV(classifier, \n",
    "                                    param_distributions = params, \n",
    "                                    n_iter=10, \n",
    "                                    scoring =\"accuracy\", n_jobs =-1, cv=10, verbose =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   46.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score=nan,\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                           callbacks=None, colsample_bylevel=1,\n",
       "                                           colsample_bynode=1,\n",
       "                                           colsample_bytree=0.7,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric='auc', gamma=0.2,\n",
       "                                           gpu_id=-1, grow_policy='depthwise',\n",
       "                                           importance_type=None,\n",
       "                                           interaction_constraints='',\n",
       "                                           learning_rate=1,...\n",
       "                   iid='deprecated', n_iter=10, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.3, 0.4, 0.5, 0.7,\n",
       "                                                             1],\n",
       "                                        'gamma': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5,\n",
       "                                                  0.6, 0.7, 0.8, 0.9],\n",
       "                                        'learning_rate': [0.01, 0.1, 0.25, 0.5,\n",
       "                                                          1],\n",
       "                                        'max_depth': [3, 5, 10],\n",
       "                                        'min_child_weight': [1, 3, 5, 7, 10, 13,\n",
       "                                                             17, 20]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_child_weight': 1,\n",
       " 'max_depth': 5,\n",
       " 'learning_rate': 0.5,\n",
       " 'gamma': 0.2,\n",
       " 'colsample_bytree': 1}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the best params from Random search\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model with the best params we got\n",
    "classifier = xgboost.XGBClassifier(min_child_weight= 1,\n",
    "  max_depth= 5,\n",
    "  learning_rate= 0.5,\n",
    "  gamma= 0.2,\n",
    "  colsample_bytree= 1,n_estimators = 100,eval_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric='auc', gamma=0.2, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.5, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, objective='binary:logistic',\n",
       "              predictor='auto', random_state=0, reg_alpha=0, ...)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy after tuning the parameters using GridSearchCV: 0.9957\n",
      "Test accuracy after tuning the parameters using GridSearchCV: 0.9873\n"
     ]
    }
   ],
   "source": [
    "y_train_pred=classifier.predict(X_train)\n",
    "y_test_pred=classifier.predict(X_test)\n",
    "print(\"Training accuracy after tuning the parameters using GridSearchCV: {:.4f}\\nTest accuracy after tuning the parameters using GridSearchCV: {:.4f}\".format(accuracy_score(y_train, y_train_pred),\n",
    "                                                    accuracy_score(y_test, y_test_pred)))\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_r = cross_val_score(classifier, X, y, cv =10, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9904807692307692"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why is this higher? \n",
    "score_r.mean() #0.9872115384615385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only to be used on validation set\n",
    "#eval_set =[(X_val, y_val)]\n",
    "#classifier.fit(X_train,\n",
    "                  #y_train,\n",
    "                  #early_stopping_rounds=10,\n",
    "                  #eval_set=eval_set,\n",
    "                  #verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If predicting using this then train the model using only train data. \n",
    "# When doing cross validation, it is trained on X, rather than X_train. \n",
    "# Cause, CV handles the split by itself. \n",
    "#y_train_pred=classifier.predict_proba(X_train)[:,1]\n",
    "#y_test_pred=classifier.predict_proba(X_test)[:,1]\n",
    "#print(\"AUC Train: {:.4f}\\nAUC Test: {:.4f}\".format(roc_auc_score(y_train, y_train_pred),\n",
    "                                                    roc_auc_score(y_test, y_test_pred)))\n",
    "\n",
    "#y_train_pred=classifier.predict(X_train)\n",
    "#y_test_pred=classifier.predict(X_test)\n",
    "#print(\"Accuracy Train: {:.4f}\\nAccuracy Test: {:.4f}\".format(accuracy_score(y_train, y_train_pred),\n",
    "                                                    accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'use_label_encoder': False,\n",
       " 'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'callbacks': None,\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bynode': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'early_stopping_rounds': None,\n",
       " 'enable_categorical': False,\n",
       " 'eval_metric': 'auc',\n",
       " 'gamma': 0.2,\n",
       " 'gpu_id': -1,\n",
       " 'grow_policy': 'depthwise',\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': '',\n",
       " 'learning_rate': 0.5,\n",
       " 'max_bin': 256,\n",
       " 'max_cat_to_onehot': 4,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 5,\n",
       " 'max_leaves': 0,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': '()',\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': 0,\n",
       " 'num_parallel_tree': 1,\n",
       " 'predictor': 'auto',\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'sampling_method': 'uniform',\n",
       " 'scale_pos_weight': 1,\n",
       " 'subsample': 1,\n",
       " 'tree_method': 'exact',\n",
       " 'validate_parameters': 1,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2 = {\n",
    "    \"learning_rate\" : [0.25, 0.5, 1],\n",
    "    \"max_depth\" : [3,5,9],\n",
    "    \"min_child_weight\" : [1,3],\n",
    "    \"gamma\" : [0.1, 0.2, 0.5],\n",
    "    \"colsample_bytree\" : [ 0.5, 1]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed: 13.0min finished\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV( \n",
    "    estimator = classifier, param_grid = params2, scoring =\"accuracy\", n_jobs =-1, cv=5, verbose =3)\n",
    "\n",
    "grid_search = grid_search.fit(X_train,y_train)\n",
    "\n",
    "#print(model.best_score_)\n",
    "#print(model.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 1,\n",
       " 'gamma': 0.5,\n",
       " 'learning_rate': 0.5,\n",
       " 'max_depth': 9,\n",
       " 'min_child_weight': 1}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best parameters so far -- {'colsample_bytree': 0.5,\n",
    " 'gamma': 0.5,\n",
    " 'learning_rate': 0.5,\n",
    " 'max_depth': 11,\n",
    " 'min_child_weight': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model with the best params we got\n",
    "classifier = xgboost.XGBClassifier(min_child_weight= 1,\n",
    "  max_depth= 9,\n",
    "  learning_rate= 0.5,\n",
    "  gamma= 0.5,\n",
    "  colsample_bytree= 1,n_estimators = 100,eval_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric='auc', gamma=0.5, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.5, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=9, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, objective='binary:logistic',\n",
       "              predictor='auto', random_state=0, reg_alpha=0, ...)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy after tuning the parameters using GridSearchCV: 0.9978\n",
      "Test accuracy after tuning the parameters using GridSearchCV: 0.9870\n"
     ]
    }
   ],
   "source": [
    "y_train_pred=classifier.predict(X_train)\n",
    "y_test_pred=classifier.predict(X_test)\n",
    "print(\"Training accuracy after tuning the parameters using GridSearchCV: {:.4f}\\nTest accuracy after tuning the parameters using GridSearchCV: {:.4f}\".format(accuracy_score(y_train, y_train_pred),\n",
    "                                                    accuracy_score(y_test, y_test_pred)))\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Stratified Kfold validation for accuracy\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "score_g = cross_val_score(classifier, X, y, cv =skf, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9910096153846153"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_g.mean() #0.9915865384615385\n",
    "\n",
    "#score_g.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_set =[(X_val, y_val)]\n",
    "#classifier.fit(X_train,\n",
    "                  #y_train,\n",
    "                  #early_stopping_rounds=10,\n",
    "                  #eval_set=eval_set,\n",
    "                  #verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy with GridSearchCV hyperparameter tuning\n",
    "#y_train_pred=classifier.predict_proba(X_train)[:,1]\n",
    "#y_test_pred=classifier.predict_proba(X_test)[:,1]\n",
    "#print(\"AUC Train: {:.4f}\\nAUC Test: {:.4f}\".format(roc_auc_score(y_train, y_train_pred),\n",
    "                                                    roc_auc_score(y_test, y_test_pred)))\n",
    "\n",
    "\n",
    "#y_train_pred=classifier.predict(X_train)\n",
    "#y_test_pred=classifier.predict(X_test)\n",
    "#print(\"Training accuracy after tuning the parameters using GridSearchCV: {:.4f}\\nTest accuracy after tuning the parameters using GridSearchCV: {:.4f}\".format(accuracy_score(y_train, y_train_pred),\n",
    "                                                    accuracy_score(y_test, y_test_pred)))\n",
    "                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization using HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 100/100 [16:50<00:00, 10.10s/trial, best loss: -0.9936899038461539]\n"
     ]
    }
   ],
   "source": [
    "# Space\n",
    "space = {\n",
    "    'learning_rate': hp.choice('learning_rate', [ 0.01, 0.1, 0.25, 0.5, 1]),\n",
    "    'max_depth' : hp.choice('max_depth', [3,5,9,11]),\n",
    "    'min_child_weight' : hp.choice('min_child_weight', range(1,4)),\n",
    "    'gamma' : hp.choice('gamma', [i/10.0 for i in range(0,8)]),\n",
    "    'colsample_bytree' : hp.choice('colsample_bytree', [i/10.0 for i in range(3,11)])\n",
    "    #'reg_alpha' : hp.choice('reg_alpha', [0, 1e-5, 1e-2, 0.1, 1, 10, 100]), \n",
    "    #'reg_lambda' : hp.choice('reg_lambda', [1e-5, 1e-2, 0.1, 1, 10, 100])\n",
    "}\n",
    "# Set up the k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5)\n",
    "# Objective function\n",
    "def objective(params):\n",
    "    \n",
    "    xgboost = XGBClassifier(seed=42, **params)\n",
    "    scores = cross_val_score(xgboost, X_train, y_train, cv=kfold, scoring='accuracy', n_jobs=-1)\n",
    "    # Extract the best score\n",
    "    best_score = max(scores)\n",
    "    # Loss must be minimized\n",
    "    loss = - best_score\n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "# Trials to track progress\n",
    "bayes_trials = Trials()\n",
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 100, trials = bayes_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.5, 'gamma': 0.0, 'learning_rate': 0.5, 'max_depth': 11, 'min_child_weight': 1}\n"
     ]
    }
   ],
   "source": [
    "# Print the index of the best parameters\n",
    "# print(best)\n",
    "# Print the values of the best parameters\n",
    "print(space_eval(space, best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model with the best params we got\n",
    "classifier = xgboost.XGBClassifier(min_child_weight= 1,\n",
    "  max_depth= 11,\n",
    "  learning_rate= 0.5,\n",
    "  gamma= 0.,\n",
    "  colsample_bytree= 0.5,\n",
    "  n_estimators = 100,eval_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.5,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric='auc', gamma=0.0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.5, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=11, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, objective='binary:logistic',\n",
       "              predictor='auto', random_state=0, reg_alpha=0, ...)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy after tuning the parameters using Bayesian Optimizer : 0.9984\n",
      "Test accuracy after tuning the parameters using Bayesian Optimizer : 0.9880\n"
     ]
    }
   ],
   "source": [
    "y_train_pred=classifier.predict(X_train)\n",
    "y_test_pred=classifier.predict(X_test)\n",
    "print(\"Training accuracy after tuning the parameters using Bayesian Optimizer : {:.4f}\\nTest accuracy after tuning the parameters using Bayesian Optimizer : {:.4f}\".format(accuracy_score(y_train, y_train_pred),\n",
    "                                                    accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Stratified Kfold validation for accuracy\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "score_b = cross_val_score(classifier, X, y, cv =skf, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9915865384615385"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_b.mean()\n",
    "#score_b.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_set =[(X_val, y_val)]\n",
    "#classifier.fit(X_train,\n",
    "                  #y_train,\n",
    "                  #early_stopping_rounds=10,\n",
    "                  #eval_set=eval_set,\n",
    "                  #verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_pred=classifier.predict_proba(X_train)[:,1]\n",
    "#y_test_pred=classifier.predict_proba(X_test)[:,1]\n",
    "#print(\"AUC Train: {:.4f}\\nAUC Test: {:.4f}\".format(roc_auc_score(y_train, y_train_pred),roc_auc_score(y_test, y_test_pred)))\n",
    "                                                    \n",
    "\n",
    "\n",
    "#y_train_pred=classifier.predict(X_train)\n",
    "#y_test_pred=classifier.predict(X_test)\n",
    "#print(\"Training accuracy after tuning the parameters using Bayesian Optimizer : {:.4f}\\nTest accuracy after tuning the parameters using Bayesian Optimizer : {:.4f}\".format(accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)))\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tpot = {\n",
    "    \"learning_rate\" : [0.1, 0.25, 0.5, 1],\n",
    "    \"max_depth\" : [3,5,9,11],\n",
    "    \"min_child_weight\" : [1,3],\n",
    "    \"gamma\" : [0.0,0.1,0.3, 0.5, 0.6, 0.7],\n",
    "    \"colsample_bytree\" : [0.4, 0.5, 0.7, 1]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Since the input matrix is a sparse matrix, please makes sure all the operators in the customized config dictionary supports sparse matriies.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc194b73bed3494e80da1c037f62b1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=144.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tpot_classifier = TPOTClassifier(generations =10, population_size = 24, offspring_size = 12,\n",
    "                              verbosity = 2, early_stop = 12, random_state = 42,\n",
    "                              config_dict = {'xgboost.XGBClassifier' : params_tpot},\n",
    "                               cv = 5, scoring = 'accuracy') #cv = 5\n",
    "\n",
    "#skf = StratifiedKFold(n_splits=5)\n",
    "#score_tpot = cross_val_score(tpot_classifier, X, y, cv =skf, scoring = 'accuracy')\n",
    "tpot_classifier.fit(X_train, y_train) ##if cv = 5 is passed as a parameter inside TPOTClassifier then run this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tpot_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-c30627d4f146>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtpot_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#print(score_tpot.mean())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tpot_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy = tpot_classifier.score(X_test, y_test)\n",
    "print(accuracy)\n",
    "#print(score_tpot.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Since the input matrix is a sparse matrix, please makes sure all the operators in the customized config dictionary supports sparse matriies.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=84.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.9887620192307693\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.9887620192307693\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.9887620192307693\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.9888822115384615\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.9888822115384615\n",
      "\n",
      "Best pipeline: XGBClassifier(input_matrix, colsample_bytree=0.4, gamma=0.5, learning_rate=0.5, max_depth=11, min_child_weight=1)\n",
      "Warning: Since the input matrix is a sparse matrix, please makes sure all the operators in the customized config dictionary supports sparse matriies.\n",
      "Warning: Since the input matrix is a sparse matrix, please makes sure all the operators in the customized config dictionary supports sparse matriies.\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "score_tpot = cross_val_score(tpot_classifier, X, y, cv =skf, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_tpot.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
