{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the dependancies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "import re\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import sys\n",
    "import pandas.core.algorithms as algos\n",
    "pd.pandas.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Vishal\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Dataset\n",
    "#df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
    "#test_data = pd.read_csv('E:\\Fake News Detection/test.csv')\n",
    "#submit = pd.read_csv('E:\\Fake News Detection/submit.csv')\n",
    "#df = pd.read_csv('train.csv')\n",
    "df = pd.read_csv('E:\\Fake News Detection/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the train and test data across rows\n",
    "#df = pd.concat([train_data, test_data], axis = 'rows')\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author'] = df['author'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't execute the following if you are using Word lemmatization. \n",
    "      \n",
    "Because lemmatization selects the meaningful part of a wor, it's better to apply it over the \"title\" column\n",
    "and then add the lowercased \"author\" column later to form the new \"Content\" column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Stemmer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    darrell lucus House Dem Aide: We Didn’t Even S...\n",
       "1    daniel j. flynn FLYNN: Hillary Clinton, Big Wo...\n",
       "2    consortiumnews.com Why the Truth Might Get You...\n",
       "3    jessica purkiss 15 Civilians Killed In Single ...\n",
       "4    howard portnoy Iranian woman jailed for fictio...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merging the \"author\" and \"title\" columns to form \"Content\"\n",
    "df['content'] = df['author']+' '+df['title'] #+df['text']\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function \"clean\" to stem to the words in content\n",
    "stemmer = PorterStemmer()\n",
    "def clean(text):\n",
    "    text=\"\".join([re.sub('[^a-zA-Z]',' ',char) for char in text])\n",
    "    text=text.lower()\n",
    "    text=text.split()\n",
    "    text=[stemmer.stem(word) for word in text if word not in set(stopwords.words(\"english\"))]\n",
    "    text=\" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the data & label\n",
    "X = df.drop(columns='label', axis=1)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        darrell lucus house dem aide even see comey le...\n",
      "1        daniel j. flynn flynn hillary clinton big woma...\n",
      "2                 consortiumnews.com truth might get fired\n",
      "3        jessica purkiss civilian killed single u airst...\n",
      "4        howard portnoy iranian woman jailed fictional ...\n",
      "                               ...                        \n",
      "20795    jerome hudson rapper trump poster child white ...\n",
      "20796    benjamin hoffman n f l playoff schedule matchu...\n",
      "20797    michael j. de la merced and rachel abrams macy...\n",
      "20798    alex ansary nato russia hold parallel exercise...\n",
      "20799                           david swanson keep f alive\n",
      "Name: content, Length: 20800, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Lemmatization\n",
    " Here I have implemented word lemmatization and Stemmer. Only one should be executed before moving onto modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "def clean(text):\n",
    "    text=\"\".join([re.sub('[^a-zA-Z]',' ',char) for char in text])\n",
    "    text=text.lower()\n",
    "    text=text.split()\n",
    "    text=[wordnet.lemmatize(word) for word in text if word not in set(stopwords.words(\"english\"))]\n",
    "    text=\" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].apply(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    darrell lucus house dem aide even see comey le...\n",
       "1    daniel j. flynn flynn hillary clinton big woma...\n",
       "2             consortiumnews.com truth might get fired\n",
       "3    jessica purkiss civilian killed single u airst...\n",
       "4    howard portnoy iranian woman jailed fictional ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'] = df['author']+' '+df['title'] #+df['text']\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the data & label\n",
    "X = df.drop(columns='label', axis=1)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        darrell lucus house dem aide even see comey le...\n",
      "1        daniel j. flynn flynn hillary clinton big woma...\n",
      "2                 consortiumnews.com truth might get fired\n",
      "3        jessica purkiss civilian killed single u airst...\n",
      "4        howard portnoy iranian woman jailed fictional ...\n",
      "                               ...                        \n",
      "20795    jerome hudson rapper trump poster child white ...\n",
      "20796    benjamin hoffman n f l playoff schedule matchu...\n",
      "20797    michael j. de la merced and rachel abrams macy...\n",
      "20798    alex ansary nato russia hold parallel exercise...\n",
      "20799                           david swanson keep f alive\n",
      "Name: content, Length: 20800, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After using either one of Lemmatization OR Stemmer run the following cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating lables and values\n",
    "X = df['content'].values\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['darrell lucus house dem aide even see comey letter jason chaffetz tweeted'\n",
      " 'daniel j. flynn flynn hillary clinton big woman campus breitbart'\n",
      " 'consortiumnews.com truth might get fired' ...\n",
      " 'michael j. de la merced and rachel abrams macy said receive takeover approach hudson bay new york time'\n",
      " 'alex ansary nato russia hold parallel exercise balkan'\n",
      " 'david swanson keep f alive']\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800,)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the textual data into numerical data using vertorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X)\n",
    "\n",
    "X = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 19633)\t0.3898334584250517\n",
      "  (0, 16877)\t0.2434231886663421\n",
      "  (0, 11241)\t0.33838114561245775\n",
      "  (0, 10892)\t0.27186640212962043\n",
      "  (0, 9847)\t0.23066376406357905\n",
      "  (0, 8918)\t0.20553409740940168\n",
      "  (0, 6426)\t0.25787634753604916\n",
      "  (0, 4822)\t0.2800374370965301\n",
      "  (0, 4555)\t0.3349354477607908\n",
      "  (0, 3676)\t0.22972642932451354\n",
      "  (0, 3097)\t0.34215548056905637\n",
      "  (0, 443)\t0.27922090210909\n",
      "  (1, 20970)\t0.26470934209118435\n",
      "  (1, 8698)\t0.19372949495800215\n",
      "  (1, 7155)\t0.726569378492845\n",
      "  (1, 4517)\t0.2704004256207564\n",
      "  (1, 3492)\t0.19421743536442002\n",
      "  (1, 2793)\t0.373249665793938\n",
      "  (1, 2401)\t0.15787930205664566\n",
      "  (1, 1930)\t0.29902641324955254\n",
      "  (2, 19548)\t0.397654988300042\n",
      "  (2, 12050)\t0.4717261145949637\n",
      "  (2, 7716)\t0.3412820420603414\n",
      "  (2, 7018)\t0.4651214075065619\n",
      "  (2, 3912)\t0.44062273209258024\n",
      "  :\t:\n",
      "  (20797, 15418)\t0.3098813010103687\n",
      "  (20797, 15155)\t0.24267060114886227\n",
      "  (20797, 12847)\t0.07857340412208622\n",
      "  (20797, 12015)\t0.17062859352394522\n",
      "  (20797, 11942)\t0.2887777850945119\n",
      "  (20797, 11341)\t0.3534778107421481\n",
      "  (20797, 10575)\t0.2182065591202005\n",
      "  (20797, 8955)\t0.21308890823360954\n",
      "  (20797, 4602)\t0.20697384611290176\n",
      "  (20797, 1683)\t0.3278394365812347\n",
      "  (20797, 962)\t0.3022010624203213\n",
      "  (20797, 741)\t0.13320514620891885\n",
      "  (20797, 123)\t0.2904219826803442\n",
      "  (20798, 16399)\t0.2219337418150095\n",
      "  (20798, 13741)\t0.4426629139282138\n",
      "  (20798, 12697)\t0.31682430375528015\n",
      "  (20798, 8778)\t0.33845240816277616\n",
      "  (20798, 6516)\t0.4097881595671415\n",
      "  (20798, 1502)\t0.4426629139282138\n",
      "  (20798, 825)\t0.30941787434402834\n",
      "  (20798, 538)\t0.2832862064819816\n",
      "  (20799, 18595)\t0.5641488296067481\n",
      "  (20799, 10230)\t0.4705790517031174\n",
      "  (20799, 4581)\t0.37686542983519666\n",
      "  (20799, 570)\t0.5641488296067481\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting test and train data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y, random_state=42)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the training data :  0.9882211538461538\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score on the training data\n",
    "X_train_prediction = model.predict(X_train)\n",
    "training_data_accuracy = accuracy_score(X_train_prediction, y_train)\n",
    "print('Accuracy score of the training data : ', training_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the testing data : 0.973798076923077\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score on test data\n",
    "X_test_pred = model.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(X_test_pred, y_test)\n",
    "print ('Accuracy score of the testing data :', test_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9699519230769231"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Implementing Bagging ensemble on Logistic Regression Classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagging_clf = BaggingClassifier(base_estimator = LogisticRegression(), n_estimators = 100, max_features = 15000, random_state = 42)\n",
    "bagging_clf.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'use_label_encoder': False,\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'callbacks': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'early_stopping_rounds': None,\n",
       " 'enable_categorical': False,\n",
       " 'eval_metric': None,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'grow_policy': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_bin': None,\n",
       " 'max_cat_to_onehot': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'max_leaves': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'predictor': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'sampling_method': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgboost = xgboost.XGBClassifier(n_estimators = 100,\n",
    "                                     eval_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "score = cross_val_score(model_xgboost, X, y, cv =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean validaiton score on 10 fold CV is : 0.9897\n",
      "The standard deviation of the spread is : 0.0016 \n"
     ]
    }
   ],
   "source": [
    "print(\"The mean validaiton score on 10 fold CV is : {:.4f}\\nThe standard deviation of the spread is : {:.4f} \"\n",
    "      .format(score.mean(),score.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning on XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting superset of parameters to train on\n",
    "params = {\n",
    "    \"learning_rate\" : [0.01, 0.10, 0.25, 0.5, 1],\n",
    "    \"max_depth\" : [3,5,10],\n",
    "    \"min_child_weight\" : [1,3,5,7, 10, 13,17, 20],\n",
    "    \"gamma\" : [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    \"colsample_bytree\" : [0.3, 0.4, 0.5, 0.7, 1],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning using Randomized Search CV\n",
    "random_search =  RandomizedSearchCV(classifier, \n",
    "                                    param_distributions = params, \n",
    "                                    n_iter=10, #random_state = 1,\n",
    "                                    scoring =\"accuracy\",  n_jobs =-1, cv=10, verbose =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score=nan,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=None, gamma=None,\n",
       "                                           gpu_id=None, grow_policy=None,\n",
       "                                           importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=N...\n",
       "                   iid='deprecated', n_iter=10, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.3, 0.4, 0.5, 0.7,\n",
       "                                                             1],\n",
       "                                        'gamma': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5,\n",
       "                                                  0.6, 0.7, 0.8, 0.9],\n",
       "                                        'learning_rate': [0.01, 0.1, 0.25, 0.5,\n",
       "                                                          1],\n",
       "                                        'max_depth': [3, 5, 10],\n",
       "                                        'min_child_weight': [1, 3, 5, 7, 10, 13,\n",
       "                                                             17, 20]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_child_weight': 1,\n",
       " 'max_depth': 10,\n",
       " 'learning_rate': 0.25,\n",
       " 'gamma': 0.4,\n",
       " 'colsample_bytree': 0.4}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the best params from Random search\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model with the best params we got\n",
    "classifier = xgboost.XGBClassifier(min_child_weight= 1,\n",
    "  max_depth= 9,\n",
    "  learning_rate= 0.5,\n",
    "  gamma= 0.7,\n",
    "  colsample_bytree= 1,n_estimators = 100,eval_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_r = cross_val_score(classifier, X, y, cv =10, scoring = 'accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean validaiton score on 10 fold CV using RandomizedSearchCV : 0.9915\n",
      "The standard deviation of the spread is : 0.0021 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"The mean validaiton score on 10 fold CV using RandomizedSearchCV : {:.4f}\\nThe standard deviation of the spread is : {:.4f} \"\n",
    "      .format(score_r.mean(),score_r.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_child_weight': 1,\n",
       " 'max_depth': 10,\n",
       " 'learning_rate': 0.25,\n",
       " 'gamma': 0.4,\n",
       " 'colsample_bytree': 0.4}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2 = {\n",
    "    \"learning_rate\" : [ 0.5, 1],\n",
    "    \"max_depth\" : [5,9, 10],\n",
    "    \"min_child_weight\" : [1,3],\n",
    "    \"gamma\" : [ 0.2, 0.5, 0.7],\n",
    "    \"colsample_bytree\" : [ 0.5, 0.7, 1]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   43.7s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 19.2min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed: 20.7min finished\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV( \n",
    "    estimator = classifier, param_grid = params2, scoring =\"accuracy\", n_jobs =-1, cv=5, verbose =3)\n",
    "\n",
    "grid_search = grid_search.fit(X,y)\n",
    "\n",
    "#print(model.best_score_)\n",
    "#print(model.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 1,\n",
       " 'gamma': 0.7,\n",
       " 'learning_rate': 0.5,\n",
       " 'max_depth': 9,\n",
       " 'min_child_weight': 1}"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model with the best params we got\n",
    "classifier = xgboost.XGBClassifier(min_child_weight= 1,\n",
    "  max_depth= 9,\n",
    "  learning_rate= 0.5,\n",
    "  gamma= 0.7,\n",
    "  colsample_bytree= 1,n_estimators = 100,eval_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Stratified Kfold validation for accuracy\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "score_g = cross_val_score(classifier, X, y, cv =skf, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean validaiton score on 10 fold CV using GridSearchCV : 0.9915\n",
      "The standard deviation of the spread is : 0.0021 \n"
     ]
    }
   ],
   "source": [
    "print(\"The mean validaiton score on 10 fold CV using GridSearchCV : {:.4f}\\nThe standard deviation of the spread is : {:.4f} \"\n",
    "      .format(score_g.mean(),score_g.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization using HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 100/100 [23:17<00:00, 13.97s/trial, best loss: -0.9935096153846154]\n"
     ]
    }
   ],
   "source": [
    "# Space\n",
    "space = {\n",
    "    'learning_rate': hp.choice('learning_rate', [ 0.01, 0.1, 0.25, 0.5, 1]),\n",
    "    'max_depth' : hp.choice('max_depth', [3,5,9,11]),\n",
    "    'min_child_weight' : hp.choice('min_child_weight', range(1,4)),\n",
    "    'gamma' : hp.choice('gamma', [i/10.0 for i in range(0,8)]),\n",
    "    'colsample_bytree' : hp.choice('colsample_bytree', [i/10.0 for i in range(3,11)])\n",
    "    #'reg_alpha' : hp.choice('reg_alpha', [0, 1e-5, 1e-2, 0.1, 1, 10, 100]), \n",
    "    #'reg_lambda' : hp.choice('reg_lambda', [1e-5, 1e-2, 0.1, 1, 10, 100])\n",
    "}\n",
    "# Set up the k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5)\n",
    "# Objective function\n",
    "def objective(params):\n",
    "    \n",
    "    xgboost = XGBClassifier(seed=42, **params)\n",
    "    scores = cross_val_score(xgboost, X, y, cv=kfold, scoring='accuracy', n_jobs=-1)\n",
    "    # Extract the best score\n",
    "    best_score = max(scores)\n",
    "    # Loss must be minimized\n",
    "    loss = - best_score\n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "# Trials to track progress\n",
    "bayes_trials = Trials()\n",
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 100, trials = bayes_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 1.0, 'gamma': 0.1, 'learning_rate': 0.5, 'max_depth': 9, 'min_child_weight': 1}\n"
     ]
    }
   ],
   "source": [
    "# Print the index of the best parameters\n",
    "# print(best)\n",
    "# Print the values of the best parameters\n",
    "print(space_eval(space, best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model with the best params we got\n",
    "classifier = xgboost.XGBClassifier(min_child_weight= 1,\n",
    "  max_depth= 9,\n",
    "  learning_rate= 0.5,\n",
    "  gamma= 0.1,\n",
    "  colsample_bytree= 1,\n",
    "  n_estimators = 100,eval_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Stratified Kfold validation for accuracy\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "score_b = cross_val_score(classifier, X, y, cv =skf, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean validaiton score on 10 fold CV using GridSearchCV : 0.9916 \n",
      "The standard deviation of the spread is : 0.0023\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean validaiton score on 10 fold CV using GridSearchCV : {:.4f} \\nThe standard deviation of the spread is : {:.4f}\"\n",
    "      .format(score_b.mean(),score_b.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tpot = {\n",
    "    \"learning_rate\" : [0.1, 0.25, 0.5, 1],\n",
    "    \"max_depth\" : [3,5,9,11],\n",
    "    \"min_child_weight\" : [1,3],\n",
    "    \"gamma\" : [0.0,0.1,0.3, 0.5, 0.6, 0.7],\n",
    "    \"colsample_bytree\" : [0.4, 0.5, 0.7, 1]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Since the input matrix is a sparse matrix, please makes sure all the operators in the customized config dictionary supports sparse matriies.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=144.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.9907211538461539\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.9907211538461539\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.9910096153846155\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.9910096153846155\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.9910096153846155\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.9910096153846155\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.9910096153846155\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.9910096153846155\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.9910096153846155\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.9910096153846155\n",
      "\n",
      "Best pipeline: XGBClassifier(CombineDFs(input_matrix, input_matrix), colsample_bytree=0.5, gamma=0.5, learning_rate=0.5, max_depth=9, min_child_weight=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict={'xgboost.XGBClassifier': {'colsample_bytree': [0.4,\n",
       "                                                                           0.5,\n",
       "                                                                           0.7,\n",
       "                                                                           1],\n",
       "                                                      'gamma': [0.0, 0.1, 0.3,\n",
       "                                                                0.5, 0.6, 0.7],\n",
       "                                                      'learning_rate': [0.1,\n",
       "                                                                        0.25,\n",
       "                                                                        0.5,\n",
       "                                                                        1],\n",
       "                                                      'max_depth': [3, 5, 9,\n",
       "                                                                    11],\n",
       "                                                      'min_child_weight': [1,\n",
       "                                                                           3]}},\n",
       "               crossover_rate=0.1, cv=5, disable_update_check=False,\n",
       "               early_stop=12, generations=10, log_file=None,\n",
       "               max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "               mutation_rate=0.9, n_jobs=1, offspring_size=12,\n",
       "               periodic_checkpoint_folder=None, population_size=24,\n",
       "               random_state=42, scoring='accuracy', subsample=1.0,\n",
       "               template=None, use_dask=False, verbosity=2, warm_start=False)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot_classifier = TPOTClassifier(generations =10, population_size = 24, offspring_size = 12,\n",
    "                              verbosity = 2, early_stop = 12, random_state = 42,\n",
    "                              config_dict = {'xgboost.XGBClassifier' : params_tpot},\n",
    "                               cv = 5, scoring = 'accuracy') #cv = 5\n",
    "\n",
    "#skf = StratifiedKFold(n_splits=5)\n",
    "#score_tpot = cross_val_score(tpot_classifier, X, y, cv =skf, scoring = 'accuracy')\n",
    "tpot_classifier.fit(X, y) ##if cv = 5 is passed as a parameter inside TPOTClassifier then run this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model with the best params we got\n",
    "classifier = xgboost.XGBClassifier(min_child_weight= 1,\n",
    "  max_depth= 9,\n",
    "  learning_rate= 0.5,\n",
    "  gamma= 0.5,\n",
    "  colsample_bytree= 0.5,\n",
    "  n_estimators = 100,eval_metric = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Stratified Kfold validation for accuracy\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "score_tpot = cross_val_score(classifier, X, y, cv =skf, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean validaiton score on 10 fold CV using TPOTClassifier : 0.9917 \n",
      "The standard deviation of the spread is : 0.0023\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean validaiton score on 10 fold CV using TPOTClassifier : {:.4f} \\nThe standard deviation of the spread is : {:.4f}\"\n",
    "      .format(score_tpot.mean(),score_tpot.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy = tpot_classifier.score(X_test, y_test)\n",
    "#print('The accuracy on test data is :' ,accuracy)\n",
    "#print(score_tpot.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skf = StratifiedKFold(n_splits=5)\n",
    "#score_tpot = cross_val_score(tpot_classifier, X, y, cv =skf, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_tpot.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'SAMME.R',\n",
       " 'base_estimator': None,\n",
       " 'learning_rate': 1.0,\n",
       " 'n_estimators': 50,\n",
       " 'random_state': None}"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaBoostClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(criterion = 'entropy',\n",
    "                              #random_state = 42,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = AdaBoostClassifier(base_estimator = tree,\n",
    "                           n_estimators = 100,\n",
    "                           algorithm = 'SAMME.R')\n",
    "                           #random_state = 42)\n",
    "\n",
    "#boost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ada = cross_val_score(boost, X, y, cv =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean validaiton score on 10 fold CV : 0.9938 \n",
      "The standard deviation of the spread is : 0.0015\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean validaiton score on 10 fold CV : {:.4f} \\nThe standard deviation of the spread is : {:.4f}\"\n",
    "      .format(score_ada.mean(),score_ada.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
